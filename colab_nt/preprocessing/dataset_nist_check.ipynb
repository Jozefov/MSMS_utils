{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZTIZdrtMW9i"
      },
      "outputs": [],
      "source": [
        "!pip install matchms\n",
        "!pip install rdkit\n",
        "!pip install torch_geometric\n",
        "!pip install pickle5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matchms.importing import load_from_msp\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "import matchms\n",
        "import pickle\n",
        "from matchms import Spectrum\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
        "\n",
        "# Pytorch and Pytorch Geometric\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "import torch.nn as nn\n",
        "\n"
      ],
      "metadata": {
        "id": "ThchEKDhMiqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7cGsjbqMitD",
        "outputId": "41883ae8-e6a2-4b07-efa5-1aaa18cd177c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/NIST\")\n",
        "BASE_DIRECTORY = \"/content/drive/MyDrive/NIST\"\n",
        "\n",
        "TRAIN_PATH = 'train.msp'\n",
        "nist_dataset_org = load_from_msp(TRAIN_PATH, metadata_harmonization=False)\n",
        "\n",
        "TEST_DATA_SIZE = 5000\n",
        "OUTPUT_SIZE = 1000\n",
        "INTENSITY_POWER = 0.5"
      ],
      "metadata": {
        "id": "WF5mC6P6Mivr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/NIST/Preprocessed_train.output\", 'rb') as handle:\n",
        "   data_list  = pickle.load(handle)"
      ],
      "metadata": {
        "id": "IxNk53UqMiya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKM5CU9wUtr2"
      },
      "outputs": [],
      "source": [
        "def mask_prediction_by_mass(total_mass, raw_prediction, index_shift):\n",
        "    # Zero out predictions to the right of the maximum possible mass.\n",
        "    # input\n",
        "    # anchor_indices: shape (,batch_size) = ex [3,4,5]\n",
        "    #     total_mass = Weights of whole molecule, not only fragment\n",
        "    # data: shape (batch_size, embedding), embedding from GNN in our case\n",
        "    # index_shift: int constant how far can heaviest fragment differ from weight of original molecule\n",
        "    #\n",
        "\n",
        "    total_mass = torch.round(total_mass).type(torch.int32)\n",
        "    indices = torch.arange(raw_prediction.shape[-1])[None, ...]\n",
        "\n",
        "    right_of_total_mass = indices > (\n",
        "            total_mass[..., None] +\n",
        "            index_shift)\n",
        "    return torch.where(right_of_total_mass, torch.zeros_like(raw_prediction),\n",
        "                        raw_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8sAYdjrUTiB"
      },
      "outputs": [],
      "source": [
        "def reverse_prediction(total_mass, raw_prediction, index_shift):\n",
        "    # reverse vector by anchor_indices and rest set to zero and make preproessing\n",
        "    # input\n",
        "    # total_mass: shape (,batch_size) = ex [3,4,5]\n",
        "    #     total_mass = Weights of whole molecule, not only fragment\n",
        "    # raw_prediction: shape (batch_size, embedding), embedding from GNN in our case\n",
        "    # index_shift: int constant how far can heaviest fragment differ from weight of original molecule\n",
        "    #     total_mass = feature_dict[fmap_constants.MOLECULE_WEIGHT][..., 0]\n",
        "\n",
        "    total_mass = torch.round(total_mass).type(torch.int32)\n",
        "    return scatter_by_anchor_indices(\n",
        "        total_mass, raw_prediction, index_shift)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBEZgIBpUTkj"
      },
      "outputs": [],
      "source": [
        "def scatter_by_anchor_indices(anchor_indices, data, index_shift):\n",
        "    # reverse vector by anchor_indices and rest set to zero\n",
        "    # input\n",
        "    # anchor_indices: shape (,batch_size) = ex [3,4,5]\n",
        "    #     total_mass = Weights of whole molecule, not only fragment\n",
        "    # data: shape (batch_size, embedding), embedding from GNN in our case\n",
        "    # index_shift: int constant how far can heaviest fragment differ from weight of original molecule\n",
        "\n",
        "    index_shift = index_shift\n",
        "    anchor_indices = anchor_indices\n",
        "    data = data.type(torch.float64)\n",
        "    batch_size = data.shape[0]\n",
        "\n",
        "    num_data_columns = data.shape[-1]\n",
        "    indices = np.arange(num_data_columns)[np.newaxis, ...]\n",
        "    shifted_indices = anchor_indices[..., None] - indices + index_shift\n",
        "    valid_indices = shifted_indices >= 0\n",
        "\n",
        "\n",
        "\n",
        "    batch_indices = torch.tile(\n",
        "          torch.arange(batch_size)[..., None], [1, num_data_columns])\n",
        "    shifted_indices += batch_indices * num_data_columns\n",
        "\n",
        "    shifted_indices = torch.reshape(shifted_indices, [-1])\n",
        "    num_elements = data.shape[0] * data.shape[1]\n",
        "    row_indices = torch.arange(num_elements)\n",
        "    stacked_indices = torch.stack([row_indices, shifted_indices], axis=1)\n",
        "\n",
        "\n",
        "    lower_batch_boundaries = torch.reshape(batch_indices * num_data_columns, [-1])\n",
        "    upper_batch_boundaries = torch.reshape(((batch_indices + 1) * num_data_columns),\n",
        "                                          [-1])\n",
        "\n",
        "    valid_indices = torch.logical_and(shifted_indices >= lower_batch_boundaries,\n",
        "                                     shifted_indices < upper_batch_boundaries)\n",
        "\n",
        "    stacked_indices = stacked_indices[valid_indices]\n",
        "\n",
        "    # num_elements[..., np.newaxis] v tf aj ked je shape (), tak vies urbit data[]\n",
        "    # teraz to z napr. 6 da na [6]\n",
        "    dense_shape = torch.tile(torch.tensor(num_elements)[..., None], [2]).type(torch.int32)\n",
        "\n",
        "    scattering_matrix = torch.sparse.FloatTensor(stacked_indices.type(torch.int64).T,\n",
        "                                                 torch.ones_like(stacked_indices[:, 0]).type(torch.float64),\n",
        "                                                dense_shape.tolist())\n",
        "\n",
        "    flattened_data = torch.reshape(data, [-1])[..., None]\n",
        "    flattened_output = torch.sparse.mm(scattering_matrix, flattened_data)\n",
        "    return torch.reshape(torch.transpose(flattened_output, 0, 1), [-1, num_data_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKprW4o6U2c7"
      },
      "outputs": [],
      "source": [
        "embedding_size = 64\n",
        "embedding_in = 32\n",
        "NODE_FEATURES = 84\n",
        "MASS_SHIFT = 5\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        # Init parent\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        # GCN layers\n",
        "        self.initial_conv = GCNConv(NODE_FEATURES, embedding_size)\n",
        "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
        "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
        "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
        "\n",
        "        self.forward_prediction = Linear(embedding_size*2, OUTPUT_SIZE)\n",
        "        self.backward_prediction = Linear(embedding_size*2, OUTPUT_SIZE)\n",
        "        self.gate = Linear(embedding_size*2, OUTPUT_SIZE)\n",
        "\n",
        "        # Output layer\n",
        "        self.out = Linear(embedding_in, OUTPUT_SIZE)\n",
        "\n",
        "    def forward(self, x, edge_index, total_mass, batch_index):\n",
        "        # First Conv layer\n",
        "        hidden = self.initial_conv(x, edge_index)\n",
        "        hidden = F.relu(hidden)\n",
        "\n",
        "        # Other Conv layers\n",
        "        hidden = self.conv1(hidden, edge_index)\n",
        "        hidden = F.relu(hidden)\n",
        "        hidden = self.conv2(hidden, edge_index)\n",
        "        hidden = F.relu(hidden)\n",
        "        hidden = self.conv3(hidden, edge_index)\n",
        "        hidden = F.relu(hidden)\n",
        "\n",
        "        # Global Pooling (stack different aggregations)\n",
        "        hidden = torch.cat([gmp(hidden, batch_index),\n",
        "                            gap(hidden, batch_index)], dim=1)\n",
        "\n",
        "        print(hidden.shape)\n",
        "\n",
        "        # Bidiractional layer\n",
        "        # Forward prediction\n",
        "        forward_prediction_hidden = self.forward_prediction(hidden)\n",
        "        forward_prediction_hidden = mask_prediction_by_mass(total_mass, forward_prediction_hidden, MASS_SHIFT)\n",
        "\n",
        "        # Backward prediction\n",
        "        backward_prediction_hidden = self.backward_prediction(hidden)\n",
        "        backward_prediction_hidden = reverse_prediction(total_mass, backward_prediction_hidden, MASS_SHIFT)\n",
        "\n",
        "        # Gate\n",
        "        gate_hidden = self.gate(hidden)\n",
        "        gate_hidden = F.sigmoid(gate_hidden)\n",
        "\n",
        "        # Apply a final (linear) classifier.\n",
        "        out = gate_hidden * forward_prediction_hidden + (1. - gate_hidden) * backward_prediction_hidden\n",
        "        print(out.shape)\n",
        "        out = F.relu(out)\n",
        "#         out = self.out(hidden)\n",
        "#         out = F.relu(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "MODEL_NAME = \"GCN_testing\"\n",
        "model = GCN()\n",
        "MODEL_SAVE = os.path.join(BASE_DIRECTORY, MODEL_NAME)\n",
        "os.makedirs(MODEL_SAVE, mode=0o777, exist_ok=True)\n",
        "print(model)\n",
        "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiQwDYbAPGoZ",
        "outputId": "82cd1d11-69cd-4098-a86f-875b98f6f325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (initial_conv): GCNConv(84, 64)\n",
            "  (conv1): GCNConv(64, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (forward_prediction): Linear(in_features=128, out_features=1000, bias=True)\n",
            "  (backward_prediction): Linear(in_features=128, out_features=1000, bias=True)\n",
            "  (gate): Linear(in_features=128, out_features=1000, bias=True)\n",
            "  (out): Linear(in_features=32, out_features=1000, bias=True)\n",
            ")\n",
            "Number of parameters:  437920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "####################################\n",
        "# HUBLER LOSS\n",
        "####################################\n",
        "\n",
        "# Root mean squared error\n",
        "loss_fn = torch.nn.HuberLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)\n",
        "\n",
        "# Use GPU for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Wrap data in a data loader\n",
        "data_size = len(data_list)\n",
        "NUM_GRAPHS_PER_BATCH = 64\n",
        "loader = DataLoader(data_list[:int(data_size * 1.0)],\n",
        "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "test_loader = DataLoader(data_list[int(data_size * 0.8):],\n",
        "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
        "\n",
        "def train(data, number_of_epoch, save_every_x_epoch):\n",
        "    print(\"Starting training...\")\n",
        "    losses = []\n",
        "    for epoch in range(number_of_epoch):\n",
        "      for batch in loader:\n",
        "          # Use GPU\n",
        "          batch.to(device)\n",
        "          # Reset gradients\n",
        "          optimizer.zero_grad()\n",
        "          # Passing the node features and the connection info\n",
        "          pred, embedding = model(batch.x.float(), batch.edge_index, batch.molecular_weight, batch.batch)\n",
        "          # Calculating the loss and gradients\n",
        "          loss = loss_fn(pred, batch.y)\n",
        "          loss.backward()\n",
        "          # Update using the gradients\n",
        "          optimizer.step()\n",
        "\n",
        "          break\n",
        "\n",
        "          # Save model every save_every_x_epoch\n",
        "          if epoch == save_every_x_epoch:\n",
        "            SAVE_PATH = f\"{epoch}.pt\"\n",
        "\n",
        "            # Save model\n",
        "            torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, os.path.join(MODEL_SAVE, SAVE_PATH))\n",
        "\n",
        "            LOSS_FILE = f\"all_loss_until_{epoch}.output\"\n",
        "            with open(os.path.join(MODEL_SAVE, LOSS_FILE), 'wb') as fid:\n",
        "              pickle.dump(losses, fid)\n",
        "              fid.close()\n",
        "\n",
        "      break\n",
        "\n",
        "      losses.append(loss)\n",
        "\n",
        "      if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch} | Train Loss {loss}\")\n",
        "    return losses, embedding\n",
        "\n",
        "# print(\"Starting training...\")\n",
        "# losses = []\n",
        "# for epoch in range(3000):\n",
        "#     loss, h = train(data_list)\n",
        "#     losses.append(loss)\n",
        "#     if epoch % 100 == 0:\n",
        "#         print(f\"Epoch {epoch} | Train Loss {loss}\")"
      ],
      "metadata": {
        "id": "QQloKz6WPGtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_SAVE = os.path.join(BASE_DIRECTORY, MODEL_NAME)\n",
        "os.makedirs(MODEL_SAVE, mode=0o777, exist_ok=True)\n",
        "train(data_list, 2, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN7opF4KPGwa",
        "outputId": "8c0244b6-68ca-41ed-8b84-7f7508880884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "torch.Size([64, 128])\n",
            "torch.Size([64, 1000])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([], tensor([[1.3546e-01, 0.0000e+00, 6.2873e-02,  ..., 0.0000e+00, 6.1763e-03,\n",
              "          0.0000e+00],\n",
              "         [1.6911e-01, 0.0000e+00, 7.8670e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00],\n",
              "         [2.1454e-01, 1.8143e-03, 1.9634e-02,  ..., 6.7606e-04, 1.3382e-02,\n",
              "          5.8276e-05],\n",
              "         ...,\n",
              "         [1.6513e-01, 9.4535e-03, 2.1554e-02,  ..., 0.0000e+00, 2.8282e-02,\n",
              "          0.0000e+00],\n",
              "         [2.3073e-01, 0.0000e+00, 2.0955e-02,  ..., 0.0000e+00, 1.0293e-02,\n",
              "          0.0000e+00],\n",
              "         [2.9246e-01, 0.0000e+00, 9.8434e-02,  ..., 8.3160e-03, 4.6487e-04,\n",
              "          0.0000e+00]], grad_fn=<CatBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_list[0].y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO_HPiMNPGzc",
        "outputId": "2580eec5-7d1f-4eef-c23c-834550b1a6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,  0.0000,  1.3863,  3.2573,  4.3429,\n",
              "          0.0000,  0.0000,  1.0986,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,  2.1961,  2.0782,  2.1961,  2.3970,  1.3863,    -inf,\n",
              "         -0.6931,    -inf,    -inf,    -inf,    -inf,  2.6383,  3.9881,  3.6625,\n",
              "          1.0986,  2.0782,  3.6368,  1.7901, -0.6931,    -inf,    -inf,    -inf,\n",
              "            -inf,  1.9445,  4.5634,  4.2895,  3.4956,  3.0436,  0.6931,  1.3863,\n",
              "            -inf,    -inf,    -inf,    -inf,  0.0000,  2.7720,  3.9110,  4.9264,\n",
              "          4.8743,  2.9947,  2.1961,  0.6931,    -inf,  0.0000,    -inf,    -inf,\n",
              "            -inf,  1.6094,  3.3663,  3.0901,  3.7367,  5.5125,  4.4059,  3.2181,\n",
              "          1.3863,  2.1961,  0.6931,    -inf,    -inf,  0.6931,  1.0986,  0.6931,\n",
              "            -inf,  2.8320,  1.7901,  3.4002,  5.0295,  3.5545,    -inf,  3.0901,\n",
              "            -inf,    -inf,  0.0000,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "          1.9445,  1.6094,  2.7074,  4.6142,  1.9445, -0.6931,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,  0.0000,  0.6931,  1.3863,\n",
              "            -inf,  4.6531,  2.3016,  0.6931,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,  1.0986,    -inf,  6.9068,\n",
              "          4.4299,  1.9445,    -inf,    -inf,    -inf,    -inf,    -inf, -0.6931,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf, -0.6931,  1.0986,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,  0.0000,    -inf,  0.0000,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,  0.0000,\n",
              "            -inf,  0.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,  3.8058,  1.7901,  3.8493,  1.3863,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U1ieLGP0PG2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "raKkDJc2PG81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5uPkEdwvPG_u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}